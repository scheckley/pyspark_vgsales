{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark as sp\n",
    "from pyspark.sql.types import DoubleType, IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://bdb2a21-ib0.dawson.hartree.stfc.ac.uk:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=pyspark-shell>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"pyspark-shell\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.csv('data/vgsales.csv',inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16719, 16)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.count(), len(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+---------------+------------+---------+--------+--------+--------+-----------+------------+------------+------------+----------+----------+---------+------+\n",
      "|                Name|Platform|Year_of_Release|       Genre|Publisher|NA_Sales|EU_Sales|JP_Sales|Other_Sales|Global_Sales|Critic_Score|Critic_Count|User_Score|User_Count|Developer|Rating|\n",
      "+--------------------+--------+---------------+------------+---------+--------+--------+--------+-----------+------------+------------+------------+----------+----------+---------+------+\n",
      "|          Wii Sports|     Wii|           2006|      Sports| Nintendo|   41.36|   28.96|    3.77|       8.45|       82.53|          76|          51|         8|       322| Nintendo|     E|\n",
      "|   Super Mario Bros.|     NES|           1985|    Platform| Nintendo|   29.08|    3.58|    6.81|       0.77|       40.24|        null|        null|      null|      null|     null|  null|\n",
      "|      Mario Kart Wii|     Wii|           2008|      Racing| Nintendo|   15.68|   12.76|    3.79|       3.29|       35.52|          82|          73|       8.3|       709| Nintendo|     E|\n",
      "|   Wii Sports Resort|     Wii|           2009|      Sports| Nintendo|   15.61|   10.93|    3.28|       2.95|       32.77|          80|          73|         8|       192| Nintendo|     E|\n",
      "|Pokemon Red/Pokem...|      GB|           1996|Role-Playing| Nintendo|   11.27|    8.89|   10.22|        1.0|       31.37|        null|        null|      null|      null|     null|  null|\n",
      "+--------------------+--------+---------------+------------+---------+--------+--------+--------+-----------+------------+------------+------------+----------+----------+---------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Platform: string (nullable = true)\n",
      " |-- Year_of_Release: string (nullable = true)\n",
      " |-- Genre: string (nullable = true)\n",
      " |-- Publisher: string (nullable = true)\n",
      " |-- NA_Sales: double (nullable = true)\n",
      " |-- EU_Sales: double (nullable = true)\n",
      " |-- JP_Sales: double (nullable = true)\n",
      " |-- Other_Sales: double (nullable = true)\n",
      " |-- Global_Sales: double (nullable = true)\n",
      " |-- Critic_Score: integer (nullable = true)\n",
      " |-- Critic_Count: integer (nullable = true)\n",
      " |-- User_Score: string (nullable = true)\n",
      " |-- User_Count: integer (nullable = true)\n",
      " |-- Developer: string (nullable = true)\n",
      " |-- Rating: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+--------+----------+----------+\n",
      "|Name                       |Platform|User_Score|User_Count|\n",
      "+---------------------------+--------+----------+----------+\n",
      "|Wii Sports                 |Wii     |8         |322       |\n",
      "|Super Mario Bros.          |NES     |null      |null      |\n",
      "|Mario Kart Wii             |Wii     |8.3       |709       |\n",
      "|Wii Sports Resort          |Wii     |8         |192       |\n",
      "|Pokemon Red/Pokemon Blue   |GB      |null      |null      |\n",
      "|Tetris                     |GB      |null      |null      |\n",
      "|New Super Mario Bros.      |DS      |8.5       |431       |\n",
      "|Wii Play                   |Wii     |6.6       |129       |\n",
      "|New Super Mario Bros. Wii  |Wii     |8.4       |594       |\n",
      "|Duck Hunt                  |NES     |null      |null      |\n",
      "|Nintendogs                 |DS      |null      |null      |\n",
      "|Mario Kart DS              |DS      |8.6       |464       |\n",
      "|Pokemon Gold/Pokemon Silver|GB      |null      |null      |\n",
      "|Wii Fit                    |Wii     |7.7       |146       |\n",
      "|Kinect Adventures!         |X360    |6.3       |106       |\n",
      "+---------------------------+--------+----------+----------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.select(\"Name\",\"Platform\",\"User_Score\",\"User_Count\").show(15, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+\n",
      "|summary|        User_Score|        User_Count|\n",
      "+-------+------------------+------------------+\n",
      "|  count|             10015|              7590|\n",
      "|   mean|7.1250461133070315|162.22990777338603|\n",
      "| stddev|1.5000060936257986| 561.2823262473789|\n",
      "|    min|                 0|                 4|\n",
      "|    max|               tbd|             10665|\n",
      "+-------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.describe([\"User_Score\",\"User_Count\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some strings in the user_score \"tbd\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|Platform|count|\n",
      "+--------+-----+\n",
      "|     PS2| 2161|\n",
      "|      DS| 2152|\n",
      "|     PS3| 1331|\n",
      "|     Wii| 1320|\n",
      "|    X360| 1262|\n",
      "|     PSP| 1209|\n",
      "|      PS| 1197|\n",
      "|      PC|  974|\n",
      "|      XB|  824|\n",
      "|     GBA|  822|\n",
      "+--------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.groupBy(\"Platform\") \\\n",
    ".count() \\\n",
    ".orderBy(\"count\", ascending=False) \\\n",
    ".show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create new dataframe to remove Null values in User_Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition1 = (data.User_Score.isNotNull()) | (data.User_Count.isNotNull())\n",
    "condition2 = data.User_Score != \"tbd\"\n",
    "data2 = data.filter(condition1).filter(condition2)\n",
    "\n",
    "#data2.show(15,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------+--------+----------+----------+\n",
      "|Name                                    |Platform|User_Score|User_Count|\n",
      "+----------------------------------------+--------+----------+----------+\n",
      "|Zumba Fitness                           |Wii     |tbd       |null      |\n",
      "|Namco Museum: 50th Anniversary          |PS2     |tbd       |null      |\n",
      "|Zumba Fitness 2                         |Wii     |tbd       |null      |\n",
      "|uDraw Studio                            |Wii     |tbd       |null      |\n",
      "|Frogger's Adventures: Temple of the Frog|GBA     |tbd       |null      |\n",
      "|Just Dance Kids                         |Wii     |tbd       |null      |\n",
      "|Dance Dance Revolution X2               |PS2     |tbd       |null      |\n",
      "|The Incredibles                         |GBA     |tbd       |null      |\n",
      "|Who wants to be a millionaire           |PC      |tbd       |null      |\n",
      "|Tetris Worlds                           |GBA     |tbd       |null      |\n",
      "|Imagine: Teacher                        |DS      |tbd       |null      |\n",
      "|Personal Trainer: Math                  |DS      |tbd       |null      |\n",
      "|Game Party 3                            |Wii     |tbd       |null      |\n",
      "|Monsters, Inc.                          |GBA     |tbd       |null      |\n",
      "|SpongeBob's Atlantis SquarePantis       |DS      |tbd       |null      |\n",
      "|MySims Kingdom                          |DS      |tbd       |null      |\n",
      "|Moshi Monsters: Moshling Zoo            |DS      |tbd       |null      |\n",
      "|Disney Sing It: Pop Hits                |Wii     |tbd       |null      |\n",
      "|Imagine: Master Chef                    |DS      |tbd       |null      |\n",
      "|Disney Princess                         |GBA     |tbd       |null      |\n",
      "+----------------------------------------+--------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.select(\"Name\",\"Platform\",\"User_Score\",\"User_Count\").filter(data.User_Score == \"tbd\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+\n",
      "|summary|        User_Score|        User_Count|\n",
      "+-------+------------------+------------------+\n",
      "|  count|              7590|              7590|\n",
      "|   mean|7.1250461133070315|162.22990777338603|\n",
      "| stddev|1.5000060936257986| 561.2823262473789|\n",
      "|    min|                 0|                 4|\n",
      "|    max|               9.7|             10665|\n",
      "+-------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data2.select(\"Name\",\"Platform\",\"User_Score\",\"User_Count\").describe([\"User_Score\",\"User_Count\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Name', 'string'),\n",
       " ('Platform', 'string'),\n",
       " ('Year_of_Release', 'double'),\n",
       " ('Genre', 'string'),\n",
       " ('Publisher', 'string'),\n",
       " ('NA_Sales', 'double'),\n",
       " ('EU_Sales', 'double'),\n",
       " ('JP_Sales', 'double'),\n",
       " ('Other_Sales', 'double'),\n",
       " ('Global_Sales', 'double'),\n",
       " ('Critic_Score', 'double'),\n",
       " ('Critic_Count', 'int'),\n",
       " ('User_Score', 'double'),\n",
       " ('User_Count', 'double'),\n",
       " ('Developer', 'string'),\n",
       " ('Rating', 'string')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.types import DoubleType\n",
    "data2 = data2.withColumn(\"Year_of_Release\", data2[\"Year_of_Release\"].cast(DoubleType()))\n",
    "data2 = data2.withColumn(\"User_Score\", data2[\"User_Score\"].cast(DoubleType()))\n",
    "data2 = data2.withColumn(\"User_Count\", data2[\"User_Count\"].cast(DoubleType()))\n",
    "data2 = data2.withColumn(\"Critic_Score\", data2[\"Critic_Score\"].cast(DoubleType()))\n",
    "\n",
    "data2 = data2.filter(data2.Year_of_Release. isNotNull())\n",
    "data2 = data2.filter(data2.User_Score. isNotNull())\n",
    "data2 = data2.filter(data2.User_Count. isNotNull())\n",
    "data2 = data2.filter(data2.Critic_Score. isNotNull())\n",
    "\n",
    "data2.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For an example of **linear regression**, let’s see if we can predict User_Score from Year_of_Release, Global_Sales, Critic_Score, and User_Count.\n",
    "\n",
    "First let’s recode all of our predictors to be Doubles (I found that this got rid of some really gnarly errors later on)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VectorAssembler\n",
    "The next step is to get our data into a form that PySpark can create a model with. To do this we use something called a VectorAssembler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|          predictors|User_Score|\n",
      "+--------------------+----------+\n",
      "|[2006.0,82.53,76....|       8.0|\n",
      "|[2008.0,35.52,82....|       8.3|\n",
      "|[2009.0,32.77,80....|       8.0|\n",
      "|[2006.0,29.8,89.0...|       8.5|\n",
      "|[2006.0,28.92,58....|       6.6|\n",
      "+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "#Input all the features in one vector column\n",
    "assembler = VectorAssembler(inputCols=['Year_of_Release', 'Global_Sales', 'Critic_Score', 'User_Count'], outputCol = 'predictors')\n",
    "output = assembler.transform(data2)\n",
    "#Input vs Output\n",
    "finalized_data = output.select(\"predictors\",\"User_Score\")\n",
    "finalized_data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we’ve delineated what features we want our model to use as predictors so that VectorAssembler can take those columns and transform them into a single column (named “predictors”) that contains all the data we want to predict with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Name',\n",
       " 'Platform',\n",
       " 'Year_of_Release',\n",
       " 'Genre',\n",
       " 'Publisher',\n",
       " 'NA_Sales',\n",
       " 'EU_Sales',\n",
       " 'JP_Sales',\n",
       " 'Other_Sales',\n",
       " 'Global_Sales',\n",
       " 'Critic_Score',\n",
       " 'Critic_Count',\n",
       " 'User_Score',\n",
       " 'User_Count',\n",
       " 'Developer',\n",
       " 'Rating',\n",
       " 'predictors']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictors = assembler.transform(data2)\n",
    "\n",
    "predictors.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What VectorAssembler.transform() does is create a new DataFrame with a new column at the end where each row contains a list of all the features we included in the inputCols parameter when we created the assembler.\n",
    "\n",
    "The final step to getting our data ready to be used in a model is to collect the new predictions column we just made and User_Score (our target variable) by themselves in a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+----------+\n",
      "|predictors               |User_Score|\n",
      "+-------------------------+----------+\n",
      "|[2006.0,82.53,76.0,322.0]|8.0       |\n",
      "|[2008.0,35.52,82.0,709.0]|8.3       |\n",
      "|[2009.0,32.77,80.0,192.0]|8.0       |\n",
      "|[2006.0,29.8,89.0,431.0] |8.5       |\n",
      "|[2006.0,28.92,58.0,129.0]|6.6       |\n",
      "+-------------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_data = predictors.select(\"predictors\", \"User_Score\")\n",
    "\n",
    "model_data.show(5,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+-----------------+\n",
      "|          predictors|User_Score|       prediction|\n",
      "+--------------------+----------+-----------------+\n",
      "|[1996.0,4.63,91.0...|       8.6|9.286464320388404|\n",
      "|[1997.0,0.42,91.0...|       7.8|9.317232362497577|\n",
      "|[1997.0,0.5,66.0,...|       8.2|7.749412368532717|\n",
      "|[1997.0,0.89,83.0...|       8.2|8.804989628478978|\n",
      "|[1997.0,9.72,92.0...|       9.2| 8.95105850934172|\n",
      "+--------------------+----------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "#Split training and testing data\n",
    "train_data,test_data = finalized_data.randomSplit([0.8,0.2])\n",
    "\n",
    "lr = LinearRegression(\n",
    "    featuresCol = 'predictors', \n",
    "    labelCol = 'User_Score')\n",
    "\n",
    "lrModel = lr.fit(train_data)\n",
    "\n",
    "pred = lrModel.evaluate(test_data)\n",
    "\n",
    "pred.predictions.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "eval = RegressionEvaluator(\n",
    "    labelCol=\"User_Score\", \n",
    "    predictionCol=\"prediction\", \n",
    "    metricName=\"rmse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1.098788618976735\n",
      "MSE: 1.2073364291928006\n",
      "MAE: 0.8338376547660356\n",
      "R squared: 0.4267778814047075\n"
     ]
    }
   ],
   "source": [
    "rmse = eval.evaluate(pred.predictions)\n",
    "mse = eval.evaluate(pred.predictions, {eval.metricName: \"mse\"})\n",
    "mae = eval.evaluate(pred.predictions, {eval.metricName: \"mae\"})\n",
    "r2 = eval.evaluate(pred.predictions, {eval.metricName: \"r2\"})\n",
    "\n",
    "print(\"RMSE: \" + str(rmse))\n",
    "print(\"MSE: \" + str(mse))\n",
    "print(\"MAE: \" + str(mae))\n",
    "print(\"R squared: \" + str(r2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|        User_Score|\n",
      "+-------+------------------+\n",
      "|  count|              5466|\n",
      "|   mean| 7.190998902305174|\n",
      "| stddev|1.4367173844624845|\n",
      "|    min|               0.5|\n",
      "|    max|               9.6|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is accounting for about 40% of the variation in the data. Can we do better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------+--------------------+\n",
      "|       prediction|User_Score|          predictors|\n",
      "+-----------------+----------+--------------------+\n",
      "|9.286464320388404|       8.6|[1996.0,4.63,91.0...|\n",
      "|9.317232362497577|       7.8|[1997.0,0.42,91.0...|\n",
      "|7.749412368532717|       8.2|[1997.0,0.5,66.0,...|\n",
      "|8.804989628478978|       8.2|[1997.0,0.89,83.0...|\n",
      "| 8.95105850934172|       9.2|[1997.0,9.72,92.0...|\n",
      "+-----------------+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "R Squared (R2) on test data = 0.426778\n"
     ]
    }
   ],
   "source": [
    "lr_predictions = lrModel.transform(test_data)\n",
    "lr_predictions.select(\"prediction\",\"User_Score\",\"predictors\").show(5)\n",
    "\n",
    "lr_evaluator = RegressionEvaluator(predictionCol=\"prediction\", \\\n",
    "                 labelCol=\"User_Score\",metricName=\"r2\")\n",
    "print(\"R Squared (R2) on test data = %g\" % lr_evaluator.evaluate(lr_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [-0.07532379723839125,-0.018782217809384848,0.0626767132859347,-0.00020013853607444478]\n",
      "Intercept: 154.0455647324577\n",
      "numIterations: 1\n",
      "objectiveHistory: [0.0]\n",
      "+--------------------+\n",
      "|           residuals|\n",
      "+--------------------+\n",
      "| -2.4213871994015337|\n",
      "|  -5.197568619369082|\n",
      "| -0.7680689855627811|\n",
      "| -1.8499522850728285|\n",
      "| -1.4978024441876467|\n",
      "| -0.8195014550468507|\n",
      "| 0.14077497726094634|\n",
      "|  -0.690895011526397|\n",
      "| -2.0198545590091666|\n",
      "| -0.2547593031156339|\n",
      "|-0.24926886617149613|\n",
      "| -0.7732866875882793|\n",
      "|-0.00161339990555...|\n",
      "| -0.6931675646325566|\n",
      "| -0.9011192608868779|\n",
      "| 0.09104097285197099|\n",
      "|0.042627029550283524|\n",
      "|-0.23043285570268424|\n",
      "| -0.5420660946358176|\n",
      "|  0.1691145270658918|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "RMSE: 1.113400\n",
      "r2: 0.399326\n"
     ]
    }
   ],
   "source": [
    "# Print the coefficients and intercept for linear regression\n",
    "print(\"Coefficients: %s\" % str(lrModel.coefficients))\n",
    "print(\"Intercept: %s\" % str(lrModel.intercept))\n",
    "\n",
    "# Summarize the model over the training set and print out some metrics\n",
    "trainingSummary = lrModel.summary\n",
    "print(\"numIterations: %d\" % trainingSummary.totalIterations)\n",
    "print(\"objectiveHistory: %s\" % str(trainingSummary.objectiveHistory))\n",
    "trainingSummary.residuals.show()\n",
    "print(\"RMSE: %f\" % trainingSummary.rootMeanSquaredError)\n",
    "print(\"r2: %f\" % trainingSummary.r2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1.099\n",
      "MSE: 1.207\n",
      "MAE: 0.834\n",
      "r2: 0.427\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "eval = RegressionEvaluator(\n",
    "    labelCol=\"User_Score\", \n",
    "    predictionCol=\"prediction\", \n",
    "    metricName=\"rmse\")\n",
    "# Root Mean Square Error\n",
    "rmse = eval.evaluate(pred.predictions)\n",
    "print(\"RMSE: %.3f\" % rmse)\n",
    "# Mean Square Error\n",
    "mse = eval.evaluate(pred.predictions, {eval.metricName: \"mse\"})\n",
    "print(\"MSE: %.3f\" % mse)\n",
    "# Mean Absolute Error\n",
    "mae = eval.evaluate(pred.predictions, {eval.metricName: \"mae\"})\n",
    "print(\"MAE: %.3f\" % mae)\n",
    "# r2 - coefficient of determination\n",
    "r2 = eval.evaluate(pred.predictions, {eval.metricName: \"r2\"})\n",
    "print(\"r2: %.3f\" %r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient-boosted tree regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------+--------------------+\n",
      "|       prediction|User_Score|          predictors|\n",
      "+-----------------+----------+--------------------+\n",
      "|8.646823334993483|       8.6|[1996.0,4.63,91.0...|\n",
      "|8.072095189472904|       7.8|[1997.0,0.42,91.0...|\n",
      "|7.209238675790539|       8.2|[1997.0,0.5,66.0,...|\n",
      "|8.343811832861526|       8.2|[1997.0,0.89,83.0...|\n",
      "|8.595960222631577|       9.2|[1997.0,9.72,92.0...|\n",
      "+-----------------+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import GBTRegressor\n",
    "\n",
    "gbt = GBTRegressor(featuresCol = 'predictors', labelCol = 'User_Score', maxIter=10)\n",
    "gbt_model = gbt.fit(train_data)\n",
    "gbt_predictions = gbt_model.transform(test_data)\n",
    "gbt_predictions.select('prediction', 'User_Score', 'predictors').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 1.10039\n",
      "R squared on test data = 0.425106\n"
     ]
    }
   ],
   "source": [
    "gbt_evaluator = RegressionEvaluator(\n",
    "    labelCol=\"User_Score\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = gbt_evaluator.evaluate(gbt_predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n",
    "\n",
    "gbt_evaluator = RegressionEvaluator(\n",
    "    labelCol=\"User_Score\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "rsquared = gbt_evaluator.evaluate(gbt_predictions)\n",
    "print(\"R squared on test data = %g\" % rsquared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------+--------------------+\n",
      "|       prediction|User_Score|          predictors|\n",
      "+-----------------+----------+--------------------+\n",
      "|9.286464320388404|       8.6|[1996.0,4.63,91.0...|\n",
      "|9.317232362497577|       7.8|[1997.0,0.42,91.0...|\n",
      "|7.749412368532717|       8.2|[1997.0,0.5,66.0,...|\n",
      "|8.804989628478978|       8.2|[1997.0,0.89,83.0...|\n",
      "| 8.95105850934172|       9.2|[1997.0,9.72,92.0...|\n",
      "+-----------------+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "R Squared (R2) on test data = 0.425106\n"
     ]
    }
   ],
   "source": [
    "gbt_predictions = gbt_model.transform(test_data)\n",
    "lr_predictions.select(\"prediction\",\"User_Score\",\"predictors\").show(5)\n",
    "\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "gbt_evaluator = RegressionEvaluator(predictionCol=\"prediction\", \\\n",
    "                 labelCol=\"User_Score\",metricName=\"r2\")\n",
    "print(\"R Squared (R2) on test data = %g\" % gbt_evaluator.evaluate(gbt_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
